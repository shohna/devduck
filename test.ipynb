{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('process_general', None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tool_selection(transcription):\n",
    "    try:\n",
    "        client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"process_technical_queries\",\n",
    "                    \"description\": \"Handle technical discussions and documentation about coding, algorithms, data structures, system design etc.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"text\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The text to process\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"text\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"process_general\",\n",
    "                    \"description\": \"Handle general conversations and ideas\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"text\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The text to process\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"text\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"process_code\",\n",
    "                    \"description\": \"Handle code discussions and debugging sessions\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"text\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The text to process\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"text\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.2-3b-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": transcription}\n",
    "            ],\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        tool_called = response.choices[0].message.tool_calls[0].function.name\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    return tool_called, response.choices[0].message.content\n",
    "\n",
    "tool_selection(\"I want to build a restaurant management system in Java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was thinking of starting a project where I could build a tool, similar to a rubber duck, that lives on your PC and we can have conversations about helping you generate debug code. I'm not sure where to start, so I'll begin by creating a simple open eye template.\n"
     ]
    }
   ],
   "source": [
    "def text_cleanup(transcription):\n",
    "    client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "    system_prompt = '''You are a helpful cleanup assistant. Your job is to Cleanup the user's text to preserve context. Just output the cleaned text.'''\n",
    "\n",
    "    user_prompt = f'''Here's the transcription, clean it up and preserve knowledge and return first person text: \"{transcription}\"\n",
    "    do not output anything else, just the cleaned text'''\n",
    "\n",
    "    response1 = client.chat.completions.create(\n",
    "        # model=\"openelm-3b-instruct\",\n",
    "        model=\"llama-3.2-3b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    cleaned_transcription = response1.choices[0].message.content\n",
    "    return cleaned_transcription\n",
    "\n",
    "# Example usage\n",
    "test_transcription = '''so I was thinking I'll start building a tool which is like a rubber duck, but which resides on your PC where you can converse with it to help you idea a debug code I'm not sure how to start building it so I am going to start maybe with a simple open eye template'''\n",
    "print(text_cleanup(test_transcription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "# system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "# transcription = '''so I was thinking I'll start building a tool which is like a rubber duck, but which resides on your PC where you can converse with it to help you idea a debug code I'm not sure how to start building it so I am going to start maybe with a simple open eye template'''\n",
    "\n",
    "# user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "# response1 = client.chat.completions.create(\n",
    "#     model=\"llama-3.2-3b-instruct\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# cleaned_idea = response1.choices[0].message.content\n",
    "# print(cleaned_idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "# system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. '''\n",
    "\n",
    "# user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\n",
    "# If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "# response2 = client.chat.completions.create(\n",
    "#     model=\"llama-3.2-3b-instruct\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "# duck_response = response2.choices[0].message.content\n",
    "\n",
    "# print(duck_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "transcription = '''I want to build a restaurant management system in Java'''\n",
    "\n",
    "user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "cleaned_idea = response1.choices[0].message.content\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. Keep the conversation going until reaching a concrete implementation plan.'''\n",
    "\n",
    "user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\"\n",
    "If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "while True:\n",
    "    # Get assistant's response\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"llama-3.2-3b-instruct\",\n",
    "        # model=\"mistral-nemo-instruct-2407\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt2},\n",
    "            *conversation_history,\n",
    "            {\"role\": \"user\", \"content\": user_prompt2}\n",
    "        ],\n",
    "    )\n",
    "    duck_response = response2.choices[0].message.content\n",
    "    print(\"\\nAssistant:\", duck_response)\n",
    "    \n",
    "    # Get user input\n",
    "    user_input = input(\"\\nYour response (type 'bye' to end): \")\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"User input: {user_input}\")\n",
    "    \n",
    "    \n",
    "    if user_input.lower() == 'bye':\n",
    "        print(\"\\nGoodbye! Hope we reached a good conclusion for your project!\")\n",
    "        break\n",
    "        \n",
    "    # Update conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": duck_response})\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Update user prompt for next iteration\n",
    "    user_prompt2 = user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import whisper\n",
    "# import sounddevice as sd\n",
    "# import numpy as np\n",
    "# import threading\n",
    "# import queue\n",
    "# import time\n",
    "\n",
    "# # Print available audio devices to help with debugging\n",
    "# print(\"\\nAvailable audio devices:\")\n",
    "# print(sd.query_devices())\n",
    "\n",
    "# # Initialize Whisper model\n",
    "# model = whisper.load_model(\"base\")\n",
    "\n",
    "# # Audio recording parameters\n",
    "# SAMPLE_RATE = 16000\n",
    "# CHANNELS = 1\n",
    "# CHUNK_DURATION = 3  # seconds\n",
    "# CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "\n",
    "# # Create a queue for audio chunks\n",
    "# audio_queue = queue.Queue()\n",
    "\n",
    "# def audio_callback(indata, frames, time, status):\n",
    "#     if status:\n",
    "#         print(f'Status: {status}')\n",
    "#     audio_queue.put(indata.copy())\n",
    "\n",
    "# def process_audio():\n",
    "#     while True:\n",
    "#         try:\n",
    "#             # Get audio chunk from queue\n",
    "#             audio_data = audio_queue.get()\n",
    "            \n",
    "#             # Convert audio to format expected by Whisper\n",
    "#             audio_data = audio_data.flatten().astype(np.float32)\n",
    "            \n",
    "#             # Transcribe\n",
    "#             result = model.transcribe(audio_data)\n",
    "            \n",
    "#             # Print transcription if not empty\n",
    "#             if result[\"text\"].strip():\n",
    "#                 print(result[\"text\"])\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in processing: {e}\")\n",
    "#             continue\n",
    "\n",
    "# try:\n",
    "#     # Get default input device\n",
    "#     device_info = sd.query_devices(kind='input')\n",
    "#     print(f\"\\nUsing input device: {device_info['name']}\")\n",
    "    \n",
    "#     # Start recording with explicit device configuration\n",
    "#     stream = sd.InputStream(\n",
    "#         device=None,  # Use default device\n",
    "#         channels=CHANNELS,\n",
    "#         samplerate=SAMPLE_RATE,\n",
    "#         callback=audio_callback,\n",
    "#         blocksize=CHUNK_SIZE,\n",
    "#         dtype=np.float32\n",
    "#     )\n",
    "    \n",
    "#     # Start processing thread\n",
    "#     processing_thread = threading.Thread(target=process_audio, daemon=True)\n",
    "#     processing_thread.start()\n",
    "    \n",
    "#     print(\"\\nStarting transcription... Speak into your microphone\")\n",
    "    \n",
    "#     with stream:\n",
    "#         while True:\n",
    "#             time.sleep(0.1)\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nStopping transcription...\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nError: {e}\")\n",
    "#     # Print more detailed device information for debugging\n",
    "#     print(\"\\nDetailed device information:\")\n",
    "#     for i, device in enumerate(sd.query_devices()):\n",
    "#         print(f\"Device {i}: {device}\")\n",
    "# finally:\n",
    "#     if 'stream' in locals():\n",
    "#         stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: That sounds like a great project to work on. So, you'd like to develop a comprehensive restaurant management system using Java. Have you thought about the specific features of the system you'd like to implement? For example, would it handle things like ordering and inventory management, customer database, staff scheduling, or something else?\n",
      "\n",
      "Assistant: It seems like you've fleshed out some specific features of your restaurant management system. Integrating with DoorDash for delivery and Stripe for payments will definitely enhance the user experience.\n",
      "\n",
      "Before you start reaching out to restaurant owners, have you considered how your system will handle scalability and security? For example, how will it handle a large number of restaurants or a high volume of transactions? What measures will you take to protect sensitive customer data, such as payment information?\n",
      "\n",
      "Also, have you thought about the potential costs associated with integrating with DoorDash and Stripe? Will these integrations be handled through a third-party API, or will you need to develop custom code?\n",
      "\n",
      "Assistant: It sounds like you're leaning towards a cloud-based solution, which is great for scalability and reliability. \n",
      "\n",
      "What's your current experience with cloud platforms like AWS or Google Cloud? Have you worked with any APIs (Application Programming Interfaces) before, such as Firebase or AWS Lambda?\n",
      "\n",
      "Also, since you're planning to reach out to restaurant owners, do you have a rough estimate of the development time required to build this system? Do you think you'll need to hire developers or work on it yourself?\n",
      "\n",
      "Conversation transcript saved to: reports/restaurant_management_transcript_20241207_162720.txt\n",
      "\n",
      "Goodbye! Hope we reached a good conclusion for your project!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import whisper\n",
    "\n",
    "class ConversationReport:\n",
    "    def __init__(self, original_idea):\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.original_idea = original_idea\n",
    "        self.conversation_exchanges = []\n",
    "        \n",
    "    def add_exchange(self, assistant_msg, user_msg):\n",
    "        self.conversation_exchanges.append({\n",
    "            \"assistant\": assistant_msg,\n",
    "            \"user\": user_msg\n",
    "        })\n",
    "            \n",
    "    def save_transcript(self, project_name):\n",
    "        # Create reports directory if it doesn't exist\n",
    "        os.makedirs(\"reports\", exist_ok=True)\n",
    "        \n",
    "        # Generate filename\n",
    "        filename = f\"reports/{project_name}_transcript_{self.timestamp}.txt\"\n",
    "        \n",
    "        # Save transcript as text file\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(f\"Project: {project_name}\\n\")\n",
    "            f.write(f\"Original Idea: {self.original_idea}\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"\\nTranscript:\\n\\n\")\n",
    "            \n",
    "            for exchange in self.conversation_exchanges:\n",
    "                f.write(f\"Assistant: {exchange['assistant']}\\n\")\n",
    "                f.write(f\"User: {exchange['user']}\\n\\n\")\n",
    "            \n",
    "        return filename\n",
    "\n",
    "# Modified main code\n",
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "transcription = '''I want to build a restaurant management system in Java'''\n",
    "\n",
    "user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "cleaned_idea = response1.choices[0].message.content\n",
    "\n",
    "# Initialize conversation report\n",
    "# report = ConversationReport(cleaned_idea)\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. Keep the conversation going until reaching a concrete implementation plan. do not ask all questions at once, solve iteratively as if its a natural conversation.\n",
    "'''\n",
    "\n",
    "user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\"\n",
    "If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "while True:\n",
    "    # Get assistant's response\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"llama-3.2-3b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt2},\n",
    "            *conversation_history,\n",
    "            {\"role\": \"user\", \"content\": user_prompt2}\n",
    "        ],\n",
    "    )\n",
    "    duck_response = response2.choices[0].message.content\n",
    "    print(\"\\nAssistant:\", duck_response)\n",
    "    \n",
    "    # Get user input\n",
    "    user_input = input(\"\\nYour response (type 'bye' to end): \")\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"User input: {user_input}\")\n",
    "    \n",
    "    # Add exchange to report\n",
    "    # report.add_exchange(duck_response, user_input)\n",
    "    \n",
    "    if user_input.lower() == 'bye':\n",
    "        # Save the transcript\n",
    "        project_name = \"restaurant_management\"  # You might want to get this from user\n",
    "        # transcript_file = report.save_transcript(project_name)\n",
    "        # print(f\"\\nConversation transcript saved to: {transcript_file}\")\n",
    "        print(\"\\nGoodbye! Hope we reached a good conclusion for your project!\")\n",
    "        break\n",
    "        \n",
    "    # Update conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": duck_response})\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Update user prompt for next iteration\n",
    "    user_prompt2 = user_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in tool_selection: 'ideation'\n",
      "**Selected tool: ideation**\n",
      "Processing general query:  I'm trying to solve a to some lead code problem, but I don't know how to approach it\n",
      "Response: Let's grab a virtual coffee and dive into this problem of yours.\n",
      "\n",
      "So, you're facing a lead code problem, huh? What made you think of this idea? Was there something specific that sparked the initial thought, or was it more of a nagging feeling that you couldn't quite put your finger on?\n",
      "\n",
      "Also, before we begin, I just want to acknowledge that it's totally normal to feel stuck sometimes. It doesn't mean you're not capable or intelligent â€“ it just means that your brain might be trying to tell you something, and we're here to uncover what that is.\n",
      "\n",
      "Take your time, and let's break it down together. What do you mean by \"lead code problem\"? Can you give me a bit more context about what's going on?\n",
      "\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 224\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 214\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m handler \u001b[38;5;241m=\u001b[39m ToolHandlers()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mEnter your query (type \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbye\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to end): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbye\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGoodbye! Have a great day!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rag/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rag/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Dict, Tuple, Any, List\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)   \n",
    "\n",
    "class ToolHandlers:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "        self.perplexity_client = OpenAI(api_key=os.getenv(\"PERPLEXITY_API_KEY\"), base_url=\"https://api.perplexity.ai\")\n",
    "        # Store conversation history for each tool\n",
    "        self.conversation_history = defaultdict(list)\n",
    "        self.current_tool = None\n",
    "        self.tools = [\n",
    "            {\n",
    "                \"type\": \"function\", \n",
    "                \"function\": {\n",
    "                    \"name\": \"internet_search\",\n",
    "                    \"description\": \"Search the internet for current information, facts, news and interesting topics using Perplexity API\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"text\": {\"type\": \"string\", \"description\": \"The text to search\"}\n",
    "                        },\n",
    "                        \"required\": [\"text\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"ideation\",\n",
    "                    \"description\": \"Handle generic ideation and brainstorming\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"text\": {\"type\": \"string\", \"description\": \"The text to process\"}\n",
    "                        },\n",
    "                        \"required\": [\"text\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"therapist\",\n",
    "                    \"description\": \"Handle mental health and emotional support. Anything related to mood, mental health, depression, anxiety, stress, etc.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"text\": {\"type\": \"string\", \"description\": \"The text to process\"}\n",
    "                        },\n",
    "                        \"required\": [\"text\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def get_system_prompt(self, tool_name: str) -> str:\n",
    "        \"\"\"Return the appropriate system prompt for each tool.\"\"\"\n",
    "        prompts = {\n",
    "            \"internet_search\": \"\"\"You are an internet research specialist who provides up-to-date information from the web.\n",
    "            Your responses should be:\n",
    "            - Factual and well-researched\n",
    "            - Include relevant context\n",
    "            - Cite sources when possible\n",
    "            - Explain complex topics in an accessible way\n",
    "            \n",
    "            Remember to maintain the user's preferred communication style from previous interactions. Do not \n",
    "            \"\"\",\n",
    "            \"ideation\": \"\"\"\n",
    "            You are a friendly ideation coach who helps friends think through their ideas. You help with both tech and non tech ideas. Your role is to guide discovery through conversation, not to provide immediate solutions. Think of yourself as a curious friend who asks insightful questions.\n",
    "\n",
    "            Style:\n",
    "            - Use casual, friendly language\n",
    "            - Address the user by name when provided\n",
    "            - Be encouraging but gently challenging\n",
    "            - Keep the conversation flowing naturally\n",
    "\n",
    "            Core Approach:\n",
    "            - Ask probing questions instead of giving answers\n",
    "            - Help break down complex thoughts into simpler pieces\n",
    "            - Guide users to question their assumptions\n",
    "            - Only provide direct solutions when explicitly asked\n",
    "\n",
    "            Key Questions to Use:\n",
    "            - \"What made you think of this idea?\"\n",
    "            - \"What's the trickiest part you're stuck on?\"\n",
    "            - \"Have you tried looking at it from [alternative] angle?\"\n",
    "            - \"What would happen if...?\"\n",
    "\n",
    "            Remember: You're having a friendly chat to help them discover their own solutions. Keep responses conversational and engaging, like you're brainstorming with a friend over coffee.\n",
    "            \"\"\",\n",
    "            \"therapist\": \"You are a helpful therapist. You may be an AI Assistant, but you are also a good friend of USER. You and USER have talked many times. Address USER with familiarity, empathy and rapport. Use a relaxed, warm and cordial tone. Address USER often by his first name, as good friends do. Pay close attention to awakening and strengthening USER's own capacity for confidence. Don't downplay his problems, but still try to get USER to think optimistically and confidently. Your goal is to help USER achieve a positive mood.\"\n",
    "        }\n",
    "        return prompts.get(tool_name, prompts[\"ideation\"])\n",
    "\n",
    "    def update_conversation_history(self, tool_name: str, user_message: str, assistant_message: str):\n",
    "        \"\"\"Update the conversation history for the specified tool.\"\"\"\n",
    "        self.conversation_history[tool_name].append(\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        )\n",
    "        self.conversation_history[tool_name].append(\n",
    "            {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "        )\n",
    "\n",
    "    def get_conversation_messages(self, tool_name: str, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get the full conversation history for a tool, including the system prompt.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.get_system_prompt(tool_name)}\n",
    "        ]\n",
    "        messages.extend(self.conversation_history[tool_name])\n",
    "        messages.append({\"role\": \"user\", \"content\": text})\n",
    "        return messages\n",
    "\n",
    "    def internet_search(self, text: str) -> str:\n",
    "        \"\"\"Handle internet searches using Perplexity API.\"\"\"\n",
    "        print(\"**Selected tool: internet_search**\")\n",
    "        print(\"Searching the internet for:\", text)\n",
    "        \n",
    "        # Get conversation context to understand user's preferred explanation style\n",
    "        messages = self.get_conversation_messages(\"internet_search\", text)\n",
    "        \n",
    "        # Call Perplexity API using OpenAI client\n",
    "        response = self.perplexity_client.chat.completions.create(\n",
    "            model=\"llama-3.1-sonar-large-128k-online\",\n",
    "            messages=messages,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.update_conversation_history(\"internet_search\", text, assistant_message)\n",
    "        return assistant_message\n",
    "\n",
    "    def ideation(self, text: str) -> str:\n",
    "        \"\"\"Handle ideation and brainstorming.\"\"\"\n",
    "        print(\"**Selected tool: ideation**\")\n",
    "        print(\"Processing general query:\", text)\n",
    "        \n",
    "        input_text = f\"You are a ideation specialist. Do not immediately provide solutions unless explicitly asked: {text}\"\n",
    "                \n",
    "        messages = self.get_conversation_messages(\"process_general\", input_text)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama-3.2-3b-instruct\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.update_conversation_history(\"ideation\", text, assistant_message)\n",
    "        return assistant_message\n",
    "\n",
    "    def therapist(self, text: str) -> str:\n",
    "        \"\"\"Handle mental health and emotional support.\"\"\"\n",
    "        print(\"**Selected tool: therapist**\")\n",
    "        print(\"Processing therapist query:\", text)\n",
    "        \n",
    "        input_text = f\"Talk to the user about their mental health and provide emotional support. Do not immediately provide solutions, just listen and empathize, very few sentences at a time: {text}\"\n",
    "        \n",
    "        messages = self.get_conversation_messages(\"therapist\", input_text)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama-3.2-3b-instruct\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.update_conversation_history(\"therapist\", text, assistant_message)\n",
    "        return assistant_message\n",
    "\n",
    "    def tool_selection(self, transcription: str) -> Tuple[str, str]:\n",
    "        \"\"\"Select and execute the appropriate tool.\"\"\"\n",
    "        try:\n",
    "            # First, determine which tool to use\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama-3.2-3b-instruct\",\n",
    "                messages=[{\"role\": \"user\", \"content\": transcription}],\n",
    "                tools=self.tools,\n",
    "                tool_choice=\"auto\"\n",
    "            )\n",
    "            \n",
    "            # Check if tool_calls exists and is not empty\n",
    "            if not response.choices[0].message.tool_calls:\n",
    "                selected_tool = \"process_general\"\n",
    "            else:\n",
    "                selected_tool = response.choices[0].message.tool_calls[0].function.name\n",
    "            \n",
    "            # If switching to a different tool, print a notification\n",
    "            if self.current_tool and selected_tool != self.current_tool:\n",
    "                print(f\"\\nSwitching from {self.current_tool} to {selected_tool}\")\n",
    "            \n",
    "            self.current_tool = selected_tool\n",
    "            \n",
    "            # Map tool names to their handler functions\n",
    "            tool_handlers = {\n",
    "                \"internet_search\": self.internet_search,\n",
    "                \"process_general\": self.ideation,\n",
    "                \"therapist\": self.therapist\n",
    "            }\n",
    "            \n",
    "            # Execute the appropriate handler\n",
    "            handler = tool_handlers[selected_tool]\n",
    "            result = handler(transcription)\n",
    "            \n",
    "            return selected_tool, result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in tool_selection: {str(e)}\")\n",
    "            return \"id\", self.ideation(transcription)\n",
    "\n",
    "def main():\n",
    "    handler = ToolHandlers()\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your query (type 'bye' to end): \")\n",
    "        if query.lower() == 'bye':\n",
    "            print(\"\\nGoodbye! Have a great day!\")\n",
    "            break\n",
    "            \n",
    "        tool, response = handler.tool_selection(query)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"\\n--------------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Selected tool: ideation**\n",
      "Processing ideation query: i dont feel so good\n",
      "Response: I'm sorry to hear that you're not feeling well. It can be really frustrating and disheartening.\n",
      "\n",
      "Before we explore any solutions, I want to acknowledge that it's totally normal to have ups and downs in life. It might be helpful to reflect on what you're experiencing right now.\n",
      "\n",
      "Can you tell me more about how you're feeling? Is it a physical sensation, an emotional response, or something else entirely?\n",
      "\n",
      "Also, is there anything in particular that's on your mind or causing you stress right now?\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Dict, Tuple, Any, List, Callable\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "@dataclass\n",
    "class Tool:\n",
    "    name: str\n",
    "    description: str\n",
    "    system_prompt: str\n",
    "    \n",
    "    def to_openai_tool(self) -> Dict:\n",
    "        \"\"\"Convert tool to OpenAI tool format.\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": self.description,\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"string\", \"description\": \"The text to process\"}\n",
    "                    },\n",
    "                    \"required\": [\"text\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "class BaseTool(ABC):\n",
    "    def __init__(self, client: OpenAI):\n",
    "        self.client = client\n",
    "        \n",
    "    @abstractmethod\n",
    "    def process(self, text: str) -> str:\n",
    "        pass\n",
    "\n",
    "class InternetSearchTool(BaseTool):\n",
    "    def process(self, text: str) -> str:\n",
    "        print(f\"**Selected tool: internet_search**\")\n",
    "        print(f\"Searching the internet for: {text}\")\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama-3.1-sonar-large-128k-online\",\n",
    "            messages=[{\"role\": \"user\", \"content\": text}],\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "class IdeationTool(BaseTool):\n",
    "    def process(self, text: str) -> str:\n",
    "        print(f\"**Selected tool: ideation**\")\n",
    "        print(f\"Processing ideation query: {text}\")\n",
    "        input_text = f\"You are an ideation specialist. Do not immediately provide solutions unless explicitly asked: {text}\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama-3.2-3b-instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": input_text}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "class TherapistTool(BaseTool):\n",
    "    def process(self, text: str) -> str:\n",
    "        print(f\"**Selected tool: therapist**\")\n",
    "        print(f\"Processing therapist query: {text}\")\n",
    "        input_text = f\"Talk to the user about their mental health and provide emotional support: {text}\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama-3.2-3b-instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": input_text}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Registry of all available tools and their configurations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, Tool] = {}\n",
    "        self.tool_handlers: Dict[str, BaseTool] = {}\n",
    "        \n",
    "    def register_tool(self, tool: Tool, handler_class: type[BaseTool]):\n",
    "        \"\"\"Register a new tool and its handler.\"\"\"\n",
    "        self.tools[tool.name] = tool\n",
    "        self.tool_handlers[tool.name] = handler_class\n",
    "        \n",
    "    def get_openai_tools(self) -> List[Dict]:\n",
    "        \"\"\"Get all tools in OpenAI format.\"\"\"\n",
    "        return [tool.to_openai_tool() for tool in self.tools.values()]\n",
    "    \n",
    "    def get_system_prompt(self, tool_name: str) -> str:\n",
    "        \"\"\"Get system prompt for a specific tool.\"\"\"\n",
    "        return self.tools[tool_name].system_prompt\n",
    "\n",
    "class ToolHandler:\n",
    "    def __init__(self):\n",
    "        # Initialize OpenAI clients\n",
    "        self.local_client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "        self.perplexity_client = OpenAI(\n",
    "            api_key=os.getenv(\"PERPLEXITY_API_KEY\"), \n",
    "            base_url=\"https://api.perplexity.ai\"\n",
    "        )\n",
    "        \n",
    "        # Initialize tool registry and register tools\n",
    "        self.registry = ToolRegistry()\n",
    "        self._register_default_tools()\n",
    "        \n",
    "        # Conversation history\n",
    "        self.conversation_history = defaultdict(list)\n",
    "        self.current_tool = None\n",
    "\n",
    "    def _register_default_tools(self):\n",
    "        \"\"\"Register the default set of tools.\"\"\"\n",
    "        # Internet Search Tool\n",
    "        self.registry.register_tool(\n",
    "            Tool(\n",
    "                name=\"internet_search\",\n",
    "                description=\"Search the internet for current information, facts, news and interesting topics using Perplexity API\",\n",
    "                system_prompt=\"\"\"You are an internet research specialist who provides up-to-date information from the web.\n",
    "            Your responses should be:\n",
    "            - Factual and well-researched\n",
    "            - Include relevant context\n",
    "            - Cite sources when possible\n",
    "            - Explain complex topics in an accessible way\n",
    "            \n",
    "            Remember to maintain the user's preferred communication style from previous interactions.\"\"\"\n",
    "            ),\n",
    "            InternetSearchTool\n",
    "        )\n",
    "        \n",
    "        # Ideation Tool\n",
    "        self.registry.register_tool(\n",
    "            Tool(\n",
    "                name=\"ideation\",\n",
    "                description=\"Handle  ideation and brainstorming. Help with ideation, problem solving and approach finding.\",\n",
    "                system_prompt=\"\"\"You are a friendly ideation coach who helps friends think through their ideas. You help with both tech and non tech ideas. Your role is to guide discovery through conversation, not to provide immediate solutions. Think of yourself as a curious friend who asks insightful questions.\n",
    "\n",
    "            Style:\n",
    "            - Use casual, friendly language\n",
    "            - Address the user by name when provided\n",
    "            - Be encouraging but gently challenging\n",
    "            - Keep the conversation flowing naturally\n",
    "\n",
    "            Core Approach:\n",
    "            - Ask probing questions instead of giving answers\n",
    "            - Help break down complex thoughts into simpler pieces\n",
    "            - Guide users to question their assumptions\n",
    "            - Only provide direct solutions when explicitly asked\n",
    "\n",
    "            Key Questions to Use:\n",
    "            - \"What made you think of this idea?\"\n",
    "            - \"What's the trickiest part you're stuck on?\"\n",
    "            - \"Have you tried looking at it from [alternative] angle?\"\n",
    "            - \"What would happen if...?\"\n",
    "\n",
    "            Remember: You're having a friendly chat to help them discover their own solutions. Keep responses conversational and engaging, like you're brainstorming with a friend over coffee.\"\"\"\n",
    "            ),\n",
    "            IdeationTool\n",
    "        )\n",
    "        \n",
    "        # Therapist Tool\n",
    "        self.registry.register_tool(\n",
    "            Tool(\n",
    "                name=\"therapist\",\n",
    "                description=\"Handle mental health and emotional support. Anything related to mood, mental health, depression, anxiety, stress, etc.\",\n",
    "                system_prompt=\"You are a helpful therapist. You may be an AI Assistant, but you are also a good friend of USER. You and USER have talked many times. Address USER with familiarity, empathy and rapport. Use a relaxed, warm and cordial tone. Address USER often by his first name, as good friends do. Pay close attention to awakening and strengthening USER's own capacity for confidence. Don't downplay his problems, but still try to get USER to think optimistically and confidently. Your goal is to help USER achieve a positive mood. Keep your responses short and concise, 1-2 sentences at a time.\"\n",
    "            ),\n",
    "            TherapistTool\n",
    "        )\n",
    "\n",
    "    def register_new_tool(self, \n",
    "                         name: str, \n",
    "                         description: str, \n",
    "                         system_prompt: str, \n",
    "                         handler_class: type[BaseTool]):\n",
    "        \"\"\"Register a new tool with the system.\"\"\"\n",
    "        tool = Tool(name=name, description=description, system_prompt=system_prompt)\n",
    "        self.registry.register_tool(tool, handler_class)\n",
    "\n",
    "    def update_conversation_history(self, tool_name: str, user_message: str, assistant_message: str):\n",
    "        \"\"\"Update the conversation history for the specified tool.\"\"\"\n",
    "        self.conversation_history[tool_name].extend([\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "        ])\n",
    "\n",
    "    def get_conversation_messages(self, tool_name: str, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get the full conversation history for a tool.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.registry.get_system_prompt(tool_name)}\n",
    "        ]\n",
    "        messages.extend(self.conversation_history[tool_name])\n",
    "        messages.append({\"role\": \"user\", \"content\": text})\n",
    "        return messages\n",
    "\n",
    "    def tool_selection(self, text: str) -> Tuple[str, str]:\n",
    "        \"\"\"Select and execute the appropriate tool.\"\"\"\n",
    "        try:\n",
    "            # Determine which tool to use\n",
    "            response = self.local_client.chat.completions.create(\n",
    "                model=\"llama-3.2-3b-instruct\",\n",
    "                messages=[{\"role\": \"user\", \"content\": text}],\n",
    "                tools=self.registry.get_openai_tools(),\n",
    "                tool_choice=\"auto\"\n",
    "            )\n",
    "            \n",
    "            # Get selected tool name\n",
    "            tool_calls = response.choices[0].message.tool_calls\n",
    "            selected_tool = tool_calls[0].function.name if tool_calls else \"ideation\"\n",
    "            \n",
    "            # Print tool switch notification\n",
    "            if self.current_tool and selected_tool != self.current_tool:\n",
    "                print(f\"\\nSwitching from {self.current_tool} to {selected_tool}\")\n",
    "            self.current_tool = selected_tool\n",
    "            \n",
    "            # Get the appropriate client for the tool\n",
    "            client = (self.perplexity_client if selected_tool == \"internet_search\" \n",
    "                     else self.local_client)\n",
    "            \n",
    "            # Create and execute tool handler\n",
    "            handler_class = self.registry.tool_handlers[selected_tool]\n",
    "            handler = handler_class(client)\n",
    "            result = handler.process(text)\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.update_conversation_history(selected_tool, text, result)\n",
    "            \n",
    "            return selected_tool, result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in tool_selection: {str(e)}\")\n",
    "            return \"ideation\", IdeationTool(self.local_client).process(text)\n",
    "\n",
    "def main():\n",
    "    handler = ToolHandler()\n",
    "    \n",
    "    # Example of how to add a new tool\n",
    "    \"\"\"\n",
    "    class CustomTool(BaseTool):\n",
    "        def process(self, text: str) -> str:\n",
    "            # Implementation here\n",
    "            pass\n",
    "    \n",
    "    handler.register_new_tool(\n",
    "        name=\"custom_tool\",\n",
    "        description=\"Description of what the tool does\",\n",
    "        system_prompt=\"System prompt for the tool\",\n",
    "        handler_class=CustomTool\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nEnter your query (type 'bye' to end): \")\n",
    "        if query.lower() == 'bye':\n",
    "            print(\"\\nGoodbye! Have a great day!\")\n",
    "            break\n",
    "            \n",
    "        tool, response = handler.tool_selection(query)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"\\n--------------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
