{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to create a tool that functions like a virtual rubber duck, but in the form of a desktop application where users can interact with it to generate debug code ideas. I'll begin by designing a simple, open-eye template as the foundation of my project.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "transcription = '''so I was thinking I'll start building a tool which is like a rubber duck, but which resides on your PC where you can converse with it to help you idea a debug code I'm not sure how to start building it so I am going to start maybe with a simple open eye template'''\n",
    "\n",
    "user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "cleaned_idea = response1.choices[0].message.content\n",
    "print(cleaned_idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds like a unique and fascinating project. I'm excited to help you brainstorm.\n",
      "\n",
      "So, let's start with the basics. What kind of interactions do you envision users having with this virtual rubber duck? Will it be more like a conversation, where the user talks to it and it responds with code ideas, or will it be more like a game, where the user plays with it and gets suggestions?\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. '''\n",
    "\n",
    "user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\n",
    "If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "duck_response = response2.choices[0].message.content\n",
    "\n",
    "print(duck_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "# system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "# transcription = '''I want to build a restaurant management system in Java'''\n",
    "\n",
    "# user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "# response1 = client.chat.completions.create(\n",
    "#     model=\"llama-3.2-3b-instruct\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# cleaned_idea = response1.choices[0].message.content\n",
    "\n",
    "# # Initialize conversation history\n",
    "# conversation_history = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\", \"content\": user_prompt}\n",
    "# ]\n",
    "\n",
    "# system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. Keep the conversation going until reaching a concrete implementation plan.'''\n",
    "\n",
    "# user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\"\n",
    "# If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "# while True:\n",
    "#     # Get assistant's response\n",
    "#     response2 = client.chat.completions.create(\n",
    "#         model=\"llama-3.2-3b-instruct\",\n",
    "#         # model=\"mistral-nemo-instruct-2407\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": system_prompt2},\n",
    "#             *conversation_history,\n",
    "#             {\"role\": \"user\", \"content\": user_prompt2}\n",
    "#         ],\n",
    "#     )\n",
    "#     duck_response = response2.choices[0].message.content\n",
    "#     print(\"\\nAssistant:\", duck_response)\n",
    "    \n",
    "#     # Get user input\n",
    "#     user_input = input(\"\\nYour response (type 'bye' to end): \")\n",
    "#     print(\"--------------------------------\")\n",
    "#     print(f\"User input: {user_input}\")\n",
    "    \n",
    "    \n",
    "#     if user_input.lower() == 'bye':\n",
    "#         print(\"\\nGoodbye! Hope we reached a good conclusion for your project!\")\n",
    "#         break\n",
    "        \n",
    "#     # Update conversation history\n",
    "#     conversation_history.append({\"role\": \"assistant\", \"content\": duck_response})\n",
    "#     conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "#     # Update user prompt for next iteration\n",
    "#     user_prompt2 = user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available audio devices:\n",
      "  0 Mrityunjay’s iPhone Microphone, Core Audio (1 in, 0 out)\n",
      "> 1 MacBook Pro Microphone, Core Audio (1 in, 0 out)\n",
      "< 2 MacBook Pro Speakers, Core Audio (0 in, 2 out)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rag/lib/python3.11/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using input device: MacBook Pro Microphone\n",
      "\n",
      "Starting transcription... Speak into your microphone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rag/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, hello.\n",
      " Can you hear me?\n",
      " whisper is working perfectly fine.\n",
      " Thank you.\n",
      " What do you mean, he won't check the trans...\n",
      " Be nice, prehistoric with our encore and from here we'll be back in Mahayana Yamag名課童 This won't change plans in a few moments Let's see... specify of course that you need to wear shadow of me Okeh, there you go To keep you healthy Take something out of mí Absolutely\n",
      " Thank you.\n",
      " Nice brief story with this.\n",
      " melaто\n",
      " புரிக்கிறாய் அம்மா.\n",
      " Thank you.\n",
      " what the\n",
      " Thank you. Thank you.\n",
      " Thank you.\n",
      " Bye Singapore.\n",
      " کے بلroads انroller ایک انہ<|cs|>\n",
      " बाहीं का बूल था है?\n",
      " We're still thinking I'm going to ask for more.\n",
      " ब१व agreeikan\n",
      "\n",
      "Stopping transcription...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Só哥 funnel Guaiubá, eu tenho sore, eu tenho sore...\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "# Print available audio devices to help with debugging\n",
    "print(\"\\nAvailable audio devices:\")\n",
    "print(sd.query_devices())\n",
    "\n",
    "# Initialize Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Audio recording parameters\n",
    "SAMPLE_RATE = 16000\n",
    "CHANNELS = 1\n",
    "CHUNK_DURATION = 3  # seconds\n",
    "CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "\n",
    "# Create a queue for audio chunks\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(f'Status: {status}')\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "def process_audio():\n",
    "    while True:\n",
    "        try:\n",
    "            # Get audio chunk from queue\n",
    "            audio_data = audio_queue.get()\n",
    "            \n",
    "            # Convert audio to format expected by Whisper\n",
    "            audio_data = audio_data.flatten().astype(np.float32)\n",
    "            \n",
    "            # Transcribe\n",
    "            result = model.transcribe(audio_data)\n",
    "            \n",
    "            # Print transcription if not empty\n",
    "            if result[\"text\"].strip():\n",
    "                print(result[\"text\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processing: {e}\")\n",
    "            continue\n",
    "\n",
    "try:\n",
    "    # Get default input device\n",
    "    device_info = sd.query_devices(kind='input')\n",
    "    print(f\"\\nUsing input device: {device_info['name']}\")\n",
    "    \n",
    "    # Start recording with explicit device configuration\n",
    "    stream = sd.InputStream(\n",
    "        device=None,  # Use default device\n",
    "        channels=CHANNELS,\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        callback=audio_callback,\n",
    "        blocksize=CHUNK_SIZE,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    # Start processing thread\n",
    "    processing_thread = threading.Thread(target=process_audio, daemon=True)\n",
    "    processing_thread.start()\n",
    "    \n",
    "    print(\"\\nStarting transcription... Speak into your microphone\")\n",
    "    \n",
    "    with stream:\n",
    "        while True:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping transcription...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    # Print more detailed device information for debugging\n",
    "    print(\"\\nDetailed device information:\")\n",
    "    for i, device in enumerate(sd.query_devices()):\n",
    "        print(f\"Device {i}: {device}\")\n",
    "finally:\n",
    "    if 'stream' in locals():\n",
    "        stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: That sounds like a great project to work on. So, you'd like to develop a comprehensive restaurant management system using Java. Have you thought about the specific features of the system you'd like to implement? For example, would it handle things like ordering and inventory management, customer database, staff scheduling, or something else?\n",
      "\n",
      "Assistant: It seems like you've fleshed out some specific features of your restaurant management system. Integrating with DoorDash for delivery and Stripe for payments will definitely enhance the user experience.\n",
      "\n",
      "Before you start reaching out to restaurant owners, have you considered how your system will handle scalability and security? For example, how will it handle a large number of restaurants or a high volume of transactions? What measures will you take to protect sensitive customer data, such as payment information?\n",
      "\n",
      "Also, have you thought about the potential costs associated with integrating with DoorDash and Stripe? Will these integrations be handled through a third-party API, or will you need to develop custom code?\n",
      "\n",
      "Assistant: It sounds like you're leaning towards a cloud-based solution, which is great for scalability and reliability. \n",
      "\n",
      "What's your current experience with cloud platforms like AWS or Google Cloud? Have you worked with any APIs (Application Programming Interfaces) before, such as Firebase or AWS Lambda?\n",
      "\n",
      "Also, since you're planning to reach out to restaurant owners, do you have a rough estimate of the development time required to build this system? Do you think you'll need to hire developers or work on it yourself?\n",
      "\n",
      "Conversation transcript saved to: reports/restaurant_management_transcript_20241207_162720.txt\n",
      "\n",
      "Goodbye! Hope we reached a good conclusion for your project!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import whisper\n",
    "\n",
    "class ConversationReport:\n",
    "    def __init__(self, original_idea):\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.original_idea = original_idea\n",
    "        self.conversation_exchanges = []\n",
    "        \n",
    "    def add_exchange(self, assistant_msg, user_msg):\n",
    "        self.conversation_exchanges.append({\n",
    "            \"assistant\": assistant_msg,\n",
    "            \"user\": user_msg\n",
    "        })\n",
    "            \n",
    "    def save_transcript(self, project_name):\n",
    "        # Create reports directory if it doesn't exist\n",
    "        os.makedirs(\"reports\", exist_ok=True)\n",
    "        \n",
    "        # Generate filename\n",
    "        filename = f\"reports/{project_name}_transcript_{self.timestamp}.txt\"\n",
    "        \n",
    "        # Save transcript as text file\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(f\"Project: {project_name}\\n\")\n",
    "            f.write(f\"Original Idea: {self.original_idea}\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"\\nTranscript:\\n\\n\")\n",
    "            \n",
    "            for exchange in self.conversation_exchanges:\n",
    "                f.write(f\"Assistant: {exchange['assistant']}\\n\")\n",
    "                f.write(f\"User: {exchange['user']}\\n\\n\")\n",
    "            \n",
    "        return filename\n",
    "\n",
    "# Modified main code\n",
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "transcription = '''I want to build a restaurant management system in Java'''\n",
    "\n",
    "user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "cleaned_idea = response1.choices[0].message.content\n",
    "\n",
    "# Initialize conversation report\n",
    "# report = ConversationReport(cleaned_idea)\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. Keep the conversation going until reaching a concrete implementation plan. do not ask all questions at once, solve iteratively as if its a natural conversation.\n",
    "'''\n",
    "\n",
    "user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\"\n",
    "If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "while True:\n",
    "    # Get assistant's response\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"llama-3.2-3b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt2},\n",
    "            *conversation_history,\n",
    "            {\"role\": \"user\", \"content\": user_prompt2}\n",
    "        ],\n",
    "    )\n",
    "    duck_response = response2.choices[0].message.content\n",
    "    print(\"\\nAssistant:\", duck_response)\n",
    "    \n",
    "    # Get user input\n",
    "    user_input = input(\"\\nYour response (type 'bye' to end): \")\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"User input: {user_input}\")\n",
    "    \n",
    "    # Add exchange to report\n",
    "    # report.add_exchange(duck_response, user_input)\n",
    "    \n",
    "    if user_input.lower() == 'bye':\n",
    "        # Save the transcript\n",
    "        project_name = \"restaurant_management\"  # You might want to get this from user\n",
    "        # transcript_file = report.save_transcript(project_name)\n",
    "        # print(f\"\\nConversation transcript saved to: {transcript_file}\")\n",
    "        print(\"\\nGoodbye! Hope we reached a good conclusion for your project!\")\n",
    "        break\n",
    "        \n",
    "    # Update conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": duck_response})\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Update user prompt for next iteration\n",
    "    user_prompt2 = user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
