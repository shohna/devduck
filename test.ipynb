{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to create a tool that functions like a virtual rubber duck, but in the form of a desktop application where users can interact with it to generate debug code ideas. I'll begin by designing a simple, open-eye template as the foundation of my project.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "transcription = '''so I was thinking I'll start building a tool which is like a rubber duck, but which resides on your PC where you can converse with it to help you idea a debug code I'm not sure how to start building it so I am going to start maybe with a simple open eye template'''\n",
    "\n",
    "user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "cleaned_idea = response1.choices[0].message.content\n",
    "print(cleaned_idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds like a unique and fascinating project. I'm excited to help you brainstorm.\n",
      "\n",
      "So, let's start with the basics. What kind of interactions do you envision users having with this virtual rubber duck? Will it be more like a conversation, where the user talks to it and it responds with code ideas, or will it be more like a game, where the user plays with it and gets suggestions?\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. '''\n",
    "\n",
    "user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\n",
    "If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "duck_response = response2.choices[0].message.content\n",
    "\n",
    "print(duck_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: It sounds like you're excited about this project. Can you tell me a bit more about what sparked your interest in building a restaurant management system? What do you think will be the most challenging part of this project, and how do you plan to approach it?\n",
      "\n",
      "Assistant: Handling race conditions is a crucial aspect of concurrent programming in Java.\n",
      "\n",
      "To approach this, let's break it down. What kind of data and operations do you envision your restaurant management system handling? For example, will it be managing tables, orders, customer information, and so on?\n",
      "\n",
      "Also, what programming concepts or techniques do you have experience with? Are you familiar with multithreading, synchronization, or concurrency models like Synchronized blocks, locks, or Atomic variables?\n",
      "\n",
      "Assistant: It sounds like your system will need to handle a variety of concurrent operations.\n",
      "\n",
      "To ensure thread safety, you'll want to use synchronization mechanisms. Here are a few approaches you could consider:\n",
      "\n",
      "1. Synchronized blocks: You can use the `synchronized` keyword to block other threads from executing certain code segments.\n",
      "2. Reentrant Locks: Java's `ReentrantLock` class provides more fine-grained control over locking mechanisms.\n",
      "3. Atomic Variables: If you're dealing with integers or other primitive types that can be safely accessed and modified concurrently, `AtomicInteger` or `AtomicLong` could be a good choice.\n",
      "\n",
      "Which of these approaches do you think would be most suitable for your system?\n",
      "\n",
      "Also, have you considered using a Thread-Local storage to store employee data, customer info and table orders?\n",
      "\n",
      "Goodbye! Hope we reached a good conclusion for your project!\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=\"lm-studio\", base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "system_prompt = '''You are a helpful assistant. Your job is to filter the user's text (which is a transcript of a conversation) and remove all filler, unnecessary and unrelated text. You must output text directly, such that it can be fed to another llm model. Fix incorrect transcripts to preserve technical knowledge. Just output '''\n",
    "\n",
    "transcription = '''I want to build a restaurant management system in Java'''\n",
    "\n",
    "user_prompt = f'''Here's the transcription, clean it up and preserve technical knowledge and return first person text: \"{transcription}\"'''\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.2-3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "cleaned_idea = response1.choices[0].message.content\n",
    "\n",
    "# Initialize conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "system_prompt2 = '''You are a helpful assistant whose job is to guide the user in ideating a project or code approach. do not supply the answer but ask questions to guide the user. Keep the conversation going until reaching a concrete implementation plan.'''\n",
    "\n",
    "user_prompt2 = f'''Here's the user's idea, filtered: \"{cleaned_idea}\"\n",
    "If you ask followup questions, make the questions normal and conversational. Do not ask all the questions at once.'''\n",
    "\n",
    "while True:\n",
    "    # Get assistant's response\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"llama-3.2-3b-instruct\",\n",
    "        # model=\"mistral-nemo-instruct-2407\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt2},\n",
    "            *conversation_history,\n",
    "            {\"role\": \"user\", \"content\": user_prompt2}\n",
    "        ],\n",
    "    )\n",
    "    duck_response = response2.choices[0].message.content\n",
    "    print(\"\\nAssistant:\", duck_response)\n",
    "    \n",
    "    # Get user input\n",
    "    user_input = input(\"\\nYour response (type 'bye' to end): \")\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"User input: {user_input}\")\n",
    "    \n",
    "    \n",
    "    if user_input.lower() == 'bye':\n",
    "        print(\"\\nGoodbye! Hope we reached a good conclusion for your project!\")\n",
    "        break\n",
    "        \n",
    "    # Update conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": duck_response})\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Update user prompt for next iteration\n",
    "    user_prompt2 = user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
